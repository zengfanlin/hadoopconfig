零、基础知识
	java基础
	web基础 - http协议 
	数据库基础 - sql
		30% -50%

一、大数据高并发基础
	1.NIO
		阻塞、非阻塞:
			从线程的角度讨论线程是否被挂起，阻塞指的是线程在某些操作过程中被挂起，不在参与cpu的争夺，直到被唤醒为知，非阻塞指的是线程在某些操作过程中不会被挂起，参与cpu的争夺，一直执行。
		同步、异步:
			从业务处理的角度讨论在执行某些操作时参与操作的多方是否要等待业务的完成才能去做其他事情，如果需要等待则为同步操作，如果不需要则为异步操作。

		BIO 同步阻塞式IO - JDK1.0 - 面向流 操作字节、字符 - 在Connect  Accept Read Write操作时都有可能造成线程阻塞 所以为阻塞，且阻塞后无法再做其他事所以是同步操作。

		!!!NIO 同步非阻塞式IO - JDK1.4 - 面向通道操作缓冲区 - 在Connect Accept Read Write操作时都不会造成线程阻塞 所以为非阻塞 但是虽然非阻塞 仍然需要编写程序 循环处理以上操作 并不能去做其他事所以是同步操作。
		
		AIO 异步非阻塞式IO - JDK7 - 大量使用回调来实现异步操作
		
		Buffer - 缓冲区 
		Channel - 通道 - 双向传输数据
			|-ServerSocketChannel
			|-SocketChannel
		Selector
		
		为什么要用NIO 优点是什么？
			传统的网络通信在服务端需要为每个客户端创建对应线程处理，而在高并发场景下，线程太多造成服务器压力很大，如果想要用少量线程来处理很多请求时，BIO阻塞式的特点限制了这个功能的实现 - 一旦在处理某个客户端的请求时 被阻塞 线程也就没有机会去处理其他客户端的请求了。所以java在jdk1.4中引入了NIO机制，实现了非阻塞式数据处理，配合额Selector 可以实现 少量线程处理很多客户端请求的情况。
		

	2.Concurrent
		jdk5提供的java高并发包，里面有大量和并发开发相关的工具类，大大的简化了java的并发开发。
		没有这个包其实所有的功能也都可以手动实现，但是会非常麻烦。

		!!!BlockingQueue - 阻塞式队列 - 解决了经典的生产者 消费者问题
		!!!ConcurrentMap - 并发map - 线程安全的map - 和HashMap HashTable的比较
		!!!CountDownLatch - 闭锁 - 在线程执行时 可以通过设置闭锁来产生阻塞 闭锁有计数器 可以在其它线程中通过调用countDown方法来使计数器减一，一旦countDownlantch中的计数器值为0 则线程被唤醒 继续执行。可以用来实现 某个线程的执行 需要其他线程达到若干条件才能继续执行的场景。
		CyclicBarrier - 栅栏 - 在多个线程中设置栅栏，先到达栅栏的线程需要等待 等到所有线程都到达栅栏，栅栏才放开让所有线程继续执行，可以用来实现控制多个线程并行程度的需求。
		Exchanger - 交换机 - 两个线程交换对象
		Semaphore - 信号量 - 控制一段重要的代码同一时间内不超过指定数量的线程并发访问。
		!!!!!!!ExecutorService - 执行器服务 - ThreadPoolExecutor 线程池 - 执行器服务器创建线程池的办法 - 创建线程池时参数的意义 - 向线程池提交任务的方法 - 关闭线程池。
		ForkJoinPool - 拆分合并执行处理
		!!!Lock - 锁 功能类似于Syncronize代码块 实现代码同步 解决线程安全问题。但是比Syncronize代码块更灵活。
			|-ReadWriteLock 读写锁 可以实现多个读不排除 有写才排斥 当大量并发读而少量修改时效率高。
		Atomic - 原子性操作
		
	3.序列化、反序列
		将内存中对象相关信息 转为字节信息的过程叫做序列化 反过来将字节信息转换回内存中对象的过程叫做反序列化
			- 持久化 和 序列化是不同的概念 持久化只的是 将 内存中易失的信息 保存到持久设备中的过程叫做持久化 序列化是对象持久化的前提 持久化是序列化的目的之一

		java的序列化、反序列化
			不能跨语言
			速度慢
			体积大	
		avro - apache
		googleProtoBuff - google
		thrift - facebook
		...
		
	4.RPC
		java远程方法调用
		并不是一项独立的技术 而是已有技术的整合 提出的解决方案 其中包括 序列化、反序列化技术 网络通信技术 等等。。

	~~以上内容 基本在面试过程中 比例大概 在5~10%比率 根据岗位不同 可能会更多
	
	5.zebra的java实现
		最开始大数据的处理就是用java开发的分布式程序
		普及分布式理念
		为了和后续学习的hadoop hive 做比较
		zebra业务需求
	
	!!!6.Zookeeper
		分布式式集群协调工具 保证了分布式集群中的数据一致性
		基于这个能力 扩展出 zk的功能：
			数据发布订阅、负载均衡、命名服务、分布式协调/通知、集群管理、分布式锁、分布式队列
		zk特点：
			顺序一致性
			原子性
			单一视图
			可靠性
			实时性 - 伪实时性、最终一致性
		zk的数据模型：
			类似与一个文件系统 同样是一个树形结构 文件系统中节点是文件夹和文件 而zk中的节点是znode 不同的是 znode可以具有子 znode 也可以同时保存数据	
			znode的类型
						普通	顺序
				持久	
				临时

		zk的shell命令：
			略
			watcher机制
		zk的安装配置：
			3台以上 奇数个机器
		zk的javaapi：
			略
		zk的常见应用场景：
			数据订阅发布
			命名服务
			分布式协调/通知
			集群管理
				注册集群机器
				任务分发
				状态汇报
				动态分配
				Master选举
				分布式锁
				分布式队列
				Barrier栅栏
		zk原理：
			过半同意 过半存活
			集群的机器数量最好是奇数个

			leader选举
	~zk的知识点零散的见于选择题和判断题 偶尔也有作为简答题考察zk的原理及应用

	7.Linux
		命令
		shell编程

	~考察的不多		
			
二、离线分析
	1.Hadoop
		~0.概述
			来源和组成
				Hadoop -  hdsf(gfs)  mapreduce(mapreduce) yarn
			目标
				海量数据的存储和处理
			安装配置
				单机模式
					不启动任何守护进程 无法使用hdfs 和 yarn 只有一个节点 用来测试mr程序
				 伪分布式
					启动守护进程 所有功能都可以使用 只有一个节点 除了只有一个节点外 和完全分布式没什么区别 主要用来开发测试 不能用在生产环境下
				 完全分布式
				 	启动守护进程 所有功能都可以使用 有多个节点 生产环境下使用
		~1.HDFS 
			上传的数据经过切块分布式存储 并且每个块都有多个备份 保证性能和可靠性

			优点：
				支持超大文件
				检测和快速应对硬件故障
				流式数据访问
				简化的一致性模型
				高容错性
				可构建在廉价机器上

			缺点：
				低延迟数据访问
				大量的小文件
				多用户写入文件、修改文件
				不支持超强的事务

			组成结构
				NameNode
					元数据的存储
				SecondaryNameNode
					帮助NameNode实现元数据的合并
				DataNode
					存储数据块(Block)
				
			!!!!!!!技术细节
				Block
					最基本的存储单位。
					128MB(64MB)
					3个副本
					如果某个块不足128MB 切出的块该是多大就是多大 128MB是块的最大大小
				NameNode
					存储的是元数据
					元数据存储在NameNode的内存和磁盘(FsImage、Edits、FsTime)中
					元数据信息包括：
						内存中的元数据：
							文件名及目录结构信息 副本数量 Block的编号 Block和DataNode的映射关系
						文件中的元数据：
							文件名及目录结构信息 副本数量 Block的编号

				SecondaryNameNode
					不是NameNode的热备 而是一个帮助者 帮助NameNode合并元数据
					触发元数据合并的条件：
						时间间隔达到：3600秒
						edits文件大小达到：64MB
					不是NameNode的热备 但是其实具有一定的备份能力，因为在SNN里保存着上一次合并的元数据信息 当NN崩溃，元数据信息丢失时，可以从SNN中找到上一次合并时的元数据 恢复出部分数据。
					但是很有可能丢失部分最新的元数据。只能作为极端工具下无奈的选择。

				DataNode
					存储Block的节点
					定时向NN发送心跳报告(3秒一次) 保持存活 向NN报告自己持有Block信息 接受NN的命令
					如果连续10分钟NN都没有收到某个DataNode心跳信息 则NN会认为DataNode已经失去连接 

				合并元数据流程
					当达到合并元数据的条件时，先产生新的edits.new文件，之后所有的更新写入此文件
					SNN此时来复制NN中的fsimage和edits文件 复制完成后开始进行合并，合并为fsimage.ckpt
					将此文件传送回NN，NN删除旧的fsimage将此文件更名为新的fsimage 再将edits.new更名为edits完成 元数据合并
					这个过程中NN并不需要停止服务，可以持续工作，利用SNN减轻了NN的合并数据的工作压力。
					

				读流程
					使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；
					Namenode会视情况返回文件的部分或者全部block列表，对于每个block，Namenode都会返回有该block拷贝的DataNode地址；
					客户端开发库Client会选取离客户端最接近的DataNode来读取block；如果客户端本身就是DataNode,那么将从本地直接获取数据.
					读取完当前block的数据后，关闭与当前的DataNode连接，并为读取下一个block寻找最佳的DataNode；
					当读完列表的block后，且文件读取还没有结束，客户端开发库会继续向Namenode获取下一批的block列表。
					读取完一个block都会进行checksum验证，如果读取datanode时出现错误，客户端会通知Namenode，然后再从下一个拥有该block拷贝的datanode继续读。
					当文件最后一个块也都读取完成后，datanode会连接namenode告知关闭文件。

				写流程
					使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；
					Namenode会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件创建一个记录，否则会让客户端抛出异常；
					当客户端开始写入文件的时候，开发库会将文件切分成多个packets，并在内部以数据队列"data queue"的形式管理这些packets，并向Namenode申请新的blocks。，获取用来存储replicas的合适的datanodes列表， 列表的大小根据在Namenode中对replication的设置而定
					开始以pipeline（管道）的形式将packet写入所 有的replicas中。开发库把packet以流的方式写入第一个datanode，该datanode把该packet存储之后，再将其传递给在此 pipeline中的下一个datanode，直到最后一个datanode，这种写数据的方式呈流水线的形式。
					最后一个datanode成功存储之后会返回一个ack packet，在pipeline里传递至客户端，在客户端的开发库内部维护着"ack queue"，成功收到datanode返回的ack packet后会从"ack queue"移除相应的packet。
					如果传输过程中，有某个datanode出现了故障，那么当前的pipeline会被关闭，出现故障的datanode会从当前的pipeline中移除， 剩余的block会继续剩下的datanode中继续以pipeline的形式传输，同时Namenode会分配一个新的datanode，保持 replicas设定的数量。
					重复如上过程将所有package都上传完成 之后 向NameNode报告成功 关闭文件

					**副本存放策略(机架感知策略)：
						一个Block有3个副本，这3个副本应该如何保存？
						客户端将第一个副本写入离自己最近的节点 如果客户端所在的节点就是个DataNode 则直接上传本地 如果不是 则找到最近的DataNode节点上传
						要通过管道流来复制Block
						此时第二个副本选择 和第一个副本不同机架的另一个DataNode节点
						第三个副本选择和第二个副本相同机架的DataNode节点

				删流程
					客户端发起请求连接NameNode表示要删除文件
					NameNode执行元数据的删除。
					当NameNode执行delete方法时，它只标记操作涉及的需要被删除的数据块，而不会主动联系这些数据块所在的DataNode节点。
					当保存着这些数据块的DataNode节点向NameNode节点发送心跳时，在心跳响应中，NameNode节点会向DataNode发出指令，要求删除这些block。
					DataNode收到指令后删除对应的Block
					所以在执行完delete方法后的一段时间内，数据块才能被真正的删除掉。

				启动流程
					NameNode启动，进入安全模式，只能查看目录结构无法进行其他操作
					开始合并元数据，注意这次合并由NameNode自己来进行，合并完成后 生成新的fsimage和新的空的edits文件
					再将fsimage中的数据恢复到内存中
					接着等待 等待DataNode的心跳报告 DataNode在心跳报告中除了告知NameNode当前DataNode存活以外，还会在心跳中 带着其所持有的Block信息
					NameNode根据DataNode心跳中的信息 在内存中构建出 Block对应的DataNode信息
					当NameNode获取到足够数量的Block信息后 解除安全模式 开始对外提供服务

					**安全模式


				shell操作

				javaapi操作


		~2.mapreduce
			基本原理
				分而治之 然后汇总
				将输入的数据 逻辑切分为多个split 每个split触发一个Mapper Mapper接受k1 v1 进过处理产生k2v2 所有k2v2经过shuffle进入reduce阶段 转换为k3v3经过reduce处理变为k4v4写出结果
				
			守护进程
				JobTracker / ResourceManager
				TaskTracker / NodeManager
				Map Reduce		

			!!!执行流程
				客户端提交一个mr的jar包给JobClient(提交方式：hadoop jar ...)
				JobClient通过RPC和ResourceManager进行通信，返回一个存放jar包的地址（HDFS）和jobId
				client将jar包写入到HDFS当中(path = hdfs上的地址 + jobId)
				开始提交任务(任务的描述信息，不是jar, 包括jobid，jar存放的位置，配置信息等等)
				ResourceManager进行初始化任务
				读取HDFS上的要处理的文件，开始计算输入分片，每一个分片对应一个MapperTask
				NodeManager通过心跳机制领取任务（任务的描述信息）
				下载所需的jar，配置文件等
				NodeManager启动一个java child子进程，用来执行具体的任务（MapperTask或ReducerTask）
				将结果写入到HDFS当中
					
			!序列化问题：
				Writable WritableComparable

			!Partitioner分区问题：
				默认分区 - hash分桶 - 具有相同k2的值一定发往同一个reducer
				自定义分区 - 
			
			!Combiner合并操作：
				相当于在map阶段执行的reduce操作 在map阶段就将数据合并一次 减少需要通过网络传输到reducer的数据量

			!!!!!!shuffle的流程：
				map阶段：
					split input map buffer spill partition sort combiner merge(partition sort combiner*)

				reduer阶段：
					fetch merge group sort reduce output

			!!Map和Reduce的数量：
				mapper的数量无法通过代码直接控制 而是由split的数量来决定的 一个split触发一个mapper
				而split的数量由对应的文件大小和切split的规则来决定
				默认情况下 切split的规则和切block的规则是一样的
				所以可以简单的认为 默认情况下 一个block对应一个split 一个split对应一个mapper

				但是，如果存在大量小文件，每个小文件都是独占一个block 对应独占一个split 对应独占一个mapper
				这样当mr一次处理大量小文件时，会创建出一对的mapper 本身数据量可能并不大 但是创建出一堆mapper 而mapper所做的事都很少 效率很低 甚至在极端情况下 大量的mapper同时创建  可能造成集群崩溃
				这种情况下 可以通过可以通过配置mapred.min.split.size来控制split的size的最小值。

				reducer的数量可以通过代码控制job.setNumReduceTasks(3);
				而reducer的数量决定了最终产生结果文件的数量

			!!!!!!常见的面试点：
				数据倾斜
				数据去重
				二次排序
				两张表实现join
				
		~3.扩展
			!!mr的输入输出控制
				
			!!!!!!hdfs的小文件处理

			!hadoop完全分布式配置		
				
	2.Flume
		海量数据 收集、聚集、传输的框架
		Apache
		Agent - Source - Channel - Sink - Event

		复杂流动 - 多级流动 扇入流动 扇出流动
		可靠性 - 事务型的数据传递，保证数据的可靠性。
		可恢复 - memoryChannel fileChannel

		Source
			AvroSource
			Spooling Directory Source	
			NetCat Source
			HTTP Source
		Sink
			Logger Sink
			File Roll Sink
			Avro Sink
			HDFS Sink	

		Channel
			Memory Channel
			File Channel
			Spillable Memory Channel 


		Selector
			在扇出操作时 默认是复制模式 扇出的多个目标节点都会收到完整数据
			可以通过设置Selector 切换为路由模式 根据Event中指定头的不同值 决定去往不同的扇出节点 从而实现数据的路由
		Interceptor
			拦截器 拦截日志 允许或不允许日志通过 或在日志通过时修改日志内容
			Timestamp Interceptor
			Host Interceptor
			Static Interceptor
			UUID Interceptor
			Search and Replace Interceptor
			Regex Filtering Interceptor
			Regex Extractor Interceptor
			
		Processor
			处理器 可以将多个sink组成一个sinkgroup组 通过processor 让 channel在一个组的多个sink上进行切换 从而实现 负载均衡 失败恢复能力

		~~flume面试 通常笔试题里没有  面试过程中可能会结合项目来问项目中的应用
		
	3.Hive
		hive是基于hadoop的数据仓库工具 
		使用hdfs存储数据 mr来处理数据 
		提供了类sql环境来进行操作

		数据库 - 数据仓库

		hive的安装配置
			元数据库 - derby mysql - DBS TBLS ColumnsV2 SDS

		hive的shell操作
			和sql非常类似 但又不完全一样 称之为HQL
			hive中的库 其实就是底层hdfs 中对应的 库名.db 的文件夹
			hive中的表 其实就是底层hdfs 中 对应 数据库文件夹下的 以表名为名字的文件夹
			hive表中的数据 其实就是在底层 表对应的文件夹中的文件
			hive对表中数据的处理 会被翻译成 mr程序执行
			hive的default库对应 /usr/hive/warehouse目录

		!!!hive的内部表 外部表
			先有表后有数据 称为内部表 表中的数据 保存到hive表对应的文件夹下 来管理
			现有数据后有表 表关联到数据所在的位置进行管理 称为外部表 数据不会拷贝到对应表对应的文件夹下
			删除表的过程中 内部表 数据跟着被删除 外部表 只删除表 表管理的数据并不会被删除

		!!!hive的分区表
			hive的表还支持分区操作
			所谓的分区操作其实就是在表对应的文件夹下 又创建出文件夹 文件夹的名称为分区的名称 将数据按照不同分区分文件夹存放
			这样未来如果需要按照分区查询数据 可以直接找到对应的文件夹 减少搜索范围 提高效率
			
		!!!hive的分桶表
			hive的表还支持分桶操作
			所谓的分桶操作其实就是将表中的数据 按照hash分桶的方式将数据 存放到多个文件中
			未来查找数据使 可以参考分桶信息 快速定位数据
			更主要的是 这其实是一种数据抽样的方法 当开发的hive的脚本需要测试使 不需要在全量数据上测试 而是通过分区进行数据抽样进行测试即可

		!!hive的内置函数
			hive本身提供的一些函数 大大提高了hive执行处理的便利性

		!!!hive的UDF
			hive的自定义函数
			可以让用户自己来开发hive的函数，使hive具有的扩展能力

			写一个类继承UDF
			自己编写一个名字为evaluate的方法 在其中编写处理代码
			打成jar包
			上传到服务器
			在hive中add jar
			在注册临时函数create temporary function fname as '类的全路径名';
			就可以使用这个自定义函数了

		hive的语法
			。。。
		hive的javaapi
			。。。

		~~考察sql的使用!!!!	
	5.Sqoop
		基于大数据的zebra实现
		flume收集日志 
		hdfs数据存储
		hive的数据清洗
		hive的数据处理
		sqoop导出数据
		结果数据的可视化

		sqoop是连接hdfs和关系型数据库的桥梁 可以双向传输数据 

		~~结合项目询问sqoop的使用
		
	6.Hbase
		基于hadoop的数据库工具
		来自于google开源的论文 BigTable
		非关系型数据库 NoSQL
		适合存储半结构化 非结构化的数据
		面向列(族)进行存储 
		性能优秀
		是使用hdfs作为存储 mapreduce作为计算引擎 zk作为集群的协调

		逻辑结构
			行键 列族 列 单元格 时间戳

		安装、配置
			单机模式
			伪分布式
			完全分布式

		shell基本操作

		javaapi操作

		!!!!!hbase原理：
			LSM树：写入快 读取可能会比较慢
		
			按行键排序 -> region分裂 -> 分布式存储、负载均衡 -> store(对应一个列族) -> memStore storeFile(HFile)	

			写的过程
				通过行键找到对应HRgion -> 根据列族找到对应store -> 将数据写入hlog 再写入memstore 返回写入成功
				如果memstore满了 创建新的memstore继续工作 旧的memstore写入到hdfs中成为hfile hfile越多 又由于不能修改 会保存有很多垃圾数据 当达到一定数据 hfile进行合并 合并时删除垃圾数据 合并后文件过大 再切分成小hfile
				如果宕机，hfile能找回来 但memstore的数据就丢失了 此时 再下次启动时 从高hlog中恢复数据
					
			读的过程
				先通过行键找到region -> 在memstore中查找 找到就直接返回 找不到就需要去寻找hfile -> 所有的hfile都需要寻找 先找hfile的Trailer找到 DataIndex信息 再找DataBlock 返回包含要寻找数据的DataBlock 多个hfile都返回 在内存中合并 找到最新版本 然后返回

			Region定位过程：
				客户端找zk 问 meta-region-server 信息 这个信息指向meta表所在的region
				客户端找meta表对应的region 通过该表查找到 要查询的表的region信息
				再连接真正需要的表的region 查找数据
				过程相对麻烦 客户端 会缓存这个信息 方便下次查找

			HFile的结构：
				Data Blocks 段–保存表中的数据，这部分可以被压缩
					Meta Blocks 段 (可选的)–保存用户自定义的kv对，可以被压缩。
					File Info 段–Hfile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。
					Data Block Index 段–Data Block的索引。每条索引的key是被索引的block的第一条记录的key。
					Meta Block Index段 (可选的)–Meta Block的索引。
					Trailer–这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先 读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。

			HBase的系统结构：
				Client
				HMaster
				HRegionserver
				ZK

			HBase和mysql的比较和hive的比较

			HBase为什么性能好

			HBase是否可以作为线上数据库使用


			!!!!!HBase表设计
				列族的设计
					尽可能的少 不要查过3个
					如果真的需要设计多个列族 要保证经常一起查询的数据要放在统一个列族中
					如果真的需要设计多个列族 要考虑列族数据不均匀的问题

				行键的设计
					基本原则
						唯一
						有意义
						字符串
						固定长度
						不宜过长 8的倍数
					最佳实践
						散列原则
						有序原则	

		~~hbase应用比较广泛 面试的时候很容易问到 集中在hbase的原理 hbase的表设计 以及在项目中的具体应用
		
	7.Phoenix
		在hbase基础上构建的sql壳

三、实时分析
	8.kafka
		大数据场景下常见的分布式消息队列
		scala语言编写

		特点：
			高吞吐量
			持久化数据存储
			分布式系统易于扩展
			客户端状态维护

		!!!概念：
			Topic 主题
			partition 分区
				分区内存放着 连续 不可变的消息 每个消息都有编号 offset 消息连续的写入和读取来保证效率
				分区内的数据 即使被消费掉也不会被删除 而是一直存在 所以kafka中的消息是可以被重复消费的 直到超时时间到来 再连续的被删除
			replication-factor 副本
				分区可以有多个副本 进行分布式存储和负载均衡
				副本有一个leader和多个follower leader负责写入和读取 follower负责同步leader 只负责读取
				一个broker可能是他持有的一部分分区的leader和另一部分分区的follower
			Producers 生产者
				
			Consumers 消费者
				消费者可以组成消费者组 
				组内竞争 组件共享

				如果想要多个消费者共享数据 放到不同的组中即可
				如果想要多个消费者竞争数据 放到同一个组中即可

		kafka的javaapi

		!!!kafka的可靠性保证
			kafka最多只能保证保证 至少一次的语义 保证不丢数据 但是可能多数据
			如果想要严格实现一次切且一次 需要自己写代码来实现去重的操作

		~~概念和原理 可靠性保证的机制 结合项目描述项目中的应用
		
	9.storm
		大数据实时计算框架

		概念：
			topology spout bolt stream
			通过spout和bolt构建出复杂的数据流处理网络

		storm的并发机制
			Node
			Worker
			Executor
			Task
			默认并发度为1 一个Node上一个Worker  一个Executor里一个Task 唯一的并发出现在Executor级别
			可以手动修改并发程度 
			
			!!数据流分组机制：
				Shuffle Grouping(随机分组)
				Fields Grouping(按字段分组)
				All Grouping(全复制分组)
				Globle Grouping(全局分组)
				None Grouping(不分组)
				Direct Grouping(指向型分组)
				Local or shuffle Grouping(本地或随机分组)

			!!可靠性保证:
				至少一次 --> tuple重发来实现 -- spout ack fail bolt 锚定 报告
				一次且一次
				按顺序处理 --> TransactionTopology


		!!!!网站流量分析
			架构图

			埋点
			Nginx反向代理
			flume日志收集
			离线分析
				hdfs存储 hive数据清洗 hive数据处理 sqoop数据导出 mysql
			实时处理
				kakfa storm	hbase mysql
			
		
四、内存计算
	10.scala
		~~面试题中不多见 如果是特别注重spark的公司可能会考
		~~如果简历上有scala是非常大的加分项 但是面试的时候可能会考察 侧重于考察 高阶函数 面向对象特点		
	11.spark
		~1.sparkCore
			内存计算 多步连续计算 数据不落地 提高效率

			!!!!RDD - 弹性分布式数据集
				所有的数据进入spark处理时 都要转换为RDD 屏蔽掉数据来源 和 数据结构的不同
				是一种分布式的数据结构 天然分布式存活 内有分区的概念 基于分区实现分布式
				数据分段 算法 血缘关系
				血缘关系保存着父RDD的信息 当RDD处理失败 丢失数据 可以通过血缘关系回溯 来重建RDD

				两种分类

				常见方法
				
			!!!!!DAG - 有向无环图
				懒惰式执行
					部分命令不会立即执行 直到不执行不行了才执行

				Transformation类型 Action类型
					T类型的方法会懒执行 - RDD调用产生新的RDD的方法
					A类型的方法不会懒执行 - RDD调用不产生新的RDD 而是产生其他影响
					懒执行的目的是为了给优化留下可能性 - 流水线优化
					
				宽依赖 窄依赖
					窄依赖 - 父RDD里每个分区都只对应子RDD中的一个分区
					宽依赖 - 父RDD里有分区都对应子RDD中的多个分区

					窄依赖不需要shuffle 而宽依赖必须shuffle

				流水线优化
					连续多个窄依赖 可以连续执行 不足走shuffle 提高效率

				DAG有向无环图的构建过程
					spark接受命令 如果是T类型的方法 都不执行 直到遇到Action类型的方法 才执行
					将所有未执行的方法和A方法 组建成DAG
					从Action方法向前回溯
					如果是窄依赖则流水线优化 如果是宽依赖则结束优化 划分为一个stage 再从当前宽依赖开始向前 重复之前过程 
					将整个DAG划分为多个stage

					DAG --> job

					Stage --> task

				spark的缓存机制
					spark 每次执行action方法 整个DAG都要重新执行 如果需要重用这个RDD效率很低
					可以设置缓存 来解决这个问题
					cache persist
			~sparkSql
				DataFrame
				通过方法来操作
				通过sql来操作
				java代码来操作
	
			~sparkStreaming
				基于spark的流式计算
				!!!可靠性保证 - 时间为单位切块 按顺序处理 又由于本质上是RDD RDD有血缘自带可靠性 
				checkpoint
				slidewindow
				输入输出控制

			~sparkMlib
				spark机器学习相关包

				房价预测

				数学建模 - 线性回归 - 预测函数 误差函数 梯度下降 - sparkMlib

			~~spark原理 sparksteaming的可靠性 spark的调优 spark在使用过程中的问题

			
五、算法、数据挖掘分析
	
六、爬虫、数据可视化
	python语言
	python中的网络基础
	pythop爬虫框架 - pyspider scrapy


		