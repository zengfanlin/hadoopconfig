一、Spark概述
	1.spark来源
		有了Hadoop为什么还要Spark：
			hadoop最初设计时 shuffle的过程中数据要频繁的落地到磁盘中 会大大的影响性能 这在最初硬件相对昂贵 内存十分宝贵是时 一种可以理解的选择
			但是随着数据量越来越大 处理的流程越来越复杂 hadoop shuflle过程中因为数据落磁盘而造成性能的低下越来越让人无法容忍 特别是 当需要连续多次mr才能完成计算时 每一次的mr都经过shuffle数据落地 性能低下的缺点就格外的明显
			所以才有了最初spark设计的目的 -- 完全基于内存进行计算 数据尽量不落地 提高效率 虽然占用能存很高 但是效率同样得到了大大的提升 可以达到hadoop的 10~100倍。
			这在硬件逐渐廉价 而数据量越来越大 的情况下优势越来越明显
		Spark特点
			用scala编写 底层是基于actor模式的akka框架 代码结构简洁
			基于DAG(有向无环图)的执行引擎 减少了计算时数据频繁读写到磁盘的开销
				**DAG有向无环图 ： spark设计之初就考虑了 大量连续计算的需求 允许在对数据处理时 经由许多步算子 按序计算来实现处理 这些处理 是一个图的结构 但是要注意的是 图有向但是不能形成环 防止死循环 这样的有向无环的处理过程就称之为Spark的DAG有向无环图。
			建立在RDD(弹性分布式数据集)之上 可以以一致的结构应对不同的大数据处理需求 进行分布式的处理
				**RDD弹性分布式数据集 是spark处理数据的基本和统一的数据结构 本质上是一种分布式的数据结构 是spark分布式执行运算的基本的单位 - 将大量的数据切分为rdd 使其可以在统一的方式 和 格式下被spark处理 内部具有分区 基于分区分布在不同的节点中 实现分布式的运算。
			提供了Cache机制来实现数据缓存进一步提高性能
			
			生态圈越来越丰富能做的事越来越多 - SparkCore SparkSql SparkStreaming GraphX MLib
			支持操作方法多，不像hadoop只有mr
			支持的语言多 Java Python Scala R
			可以使用HDFS作为存储结构 可以使用Yarn作为协调框架
			
	2.搭建Spark的单机模式
		必须安装好JDK
		下载安装包
		上传到Linux进行解压
		修改配置文件
			复制conf spark-env.sh.template 文件为 spark-env.sh
			在其中修改，增加如下内容
				SPARK_LOCAL_IP=服务器IP地址

	3.spark的使用
		在bin目录下通过 spark-shell --master=local 启动本地模式
		启动后 发现打印消息
			Spark context Web UI available at http://192.168.242.101:4040//Spark的浏览器界面
			Spark context available as 'sc' (master = local, app id = local-1490336686508).//Spark提供了环境对象 sc
			Spark session available as 'spark'.//Spark提供了会话独享spark


	4.RDD - 弹性分布式数据集
		Resilient Distributed Dataset(RDD)，弹性分布式数据集，是Spark上的一个核心抽象，表示用于并行计算的，不可修改的，对数据集合进行分区的分布式的数据结构。不同来源的数据 都可以经过转换变为RDD 再由Spark进行处理。
		这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。它是spark提供的一个特殊集合类。普通的集合数据作为一个整体，但RDD中的数据进行了分区Partition处理，这样做的目的就是为了分布式。如：传统List(1,2,3,4)是一个整体，RDD可能就是RDD(1,2) (3,4)。这样如需计算时，就把1和2发送给一个worker，把3和4发送给另一个worker。按分区完成数据的分发进行分布式运算。
		主要分为三部分组成：数据分片、计算函数、RDD依赖（lineage血缘关系）
		Lineage让RDD有了生命，可以进行向前的追溯，当某个节点计算错误时，只需要根据Lineage重新计算相关的操作而不必回滚整个程序，这点对懒执行和容错特别有意义。
		
		将内存中的普通的集合变为RDD
			sc.parallelize(List(1,2,3))
			sc.parallelize(List(1,2,3),2)//指定分区个数

			val rdd2 = sc.makeRDD( List(1,2,3,4), 2 )			
			sc.makeRDD(List(1,2,3,4,5),2);

		查看RDD
			rdd.collect //收集rdd中的数据组成Array返回 此方法将会把分布式存储的rdd中的数据集中到一台机器中组建Array 可以想象出 所有数据到一台机器内存中 严重的消耗内存 注意 在生产环境下一定要慎用这个方法 容易内存溢出
			
		查看分区结构
			spark并没有原生的提供rdd的分区查看工具 我们可以自己来写一个

			import org.apache.spark.rdd.RDD
			import scala.reflect.ClassTag
			object su {
				def debug[T: ClassTag](rdd: RDD[T]) = {
					rdd.mapPartitionsWithIndex((i: Int, iter: Iterator[T]) => {
						val m = scala.collection.mutable.Map[Int, List[T]]()
						var list = List[T]()
						while (iter.hasNext) {
							list = list :+ iter.next
						}
						m(i) = list
						m.iterator
					}).collect().foreach((x: Tuple2[Int, List[T]]) => {
						val i = x._1
						println(s"partition:[$i]")
						x._2.foreach { println }
					})
				}
			}

二、Spark中RDD的详解
	1.RDD分为两类
		PairRDD
			键值对类型的RDD
		RDD
			普通类型的RDD

	2.PairRDD提供的方法
		(1)aggregateByKey(zeroValue)(func1,func2)
			zeroValue表示初始值，初始值会参与func1的计算
			在分区内，按key分组，把每组的值进行fun1的计算
			再将每个分区每组的计算结果按fun2进行计算

			val rdd = sc.parallelize(List(("cat",2), ("dog",5),("cat",4),("dog",3),("cat",6),("dog",3),("cat",9),("dog",1)),2);
			scala> su.debug(rdd)
				partition:[0]
				(cat,2)
				(dog,5)
				(cat,4)
				(dog,3)
				partition:[1]
				(cat,6)
				(dog,3)
				(cat,9)
				(dog,1)
			scala> import scala.math._
			scala> rdd.aggregateByKey(0)(max(_,_),_+_);
				Array((dog,8), (cat,13))

		(2)groupByKey
			val rdd = sc.parallelize(List(("cat",2), ("dog",5),("cat",4),("dog",3),("cat",6),("dog",3),("cat",9),("dog",1)),2);
			rdd.groupByKey()

		(3)reduceByKey 按照键来进行合并处理
			var rdd = sc.makeRDD( List( ("hello",1),("spark",1),("hello",1),("world",1) ) )
			rdd.reduceByKey(_+_);
					
		(4)join
			val rdd1 = sc.makeRDD(List(("cat",1),("dog",2)))
			val rdd2 = sc.makeRDD(List(("cat",3),("dog",4),("tiger",9)))
			rdd1.join(rdd2);

		(5)partitionBy
			通常我们在创建RDD时不指定分区规则 将会导致 数据自动分区
			我们也可以通过partitionBy方法人为指定分区方式来进行分区
			常见的分区器有：
				HashPartitioner
				RangePartitioner

			import org.apache.spark._
			var rdd = sc.makeRDD(List((2,"aaa"),(9,"bbb"),(7,"ccc"),(9,"ddd"),(3,"eee"),(2,"fff")),2);
			rdd.partitionBy(new HashPartitioner(2))//按照键的 hash%分区数 得到的编号去往指定的分区 这种方式可以实现将相同键的数据 分发给同一个分区的效果
			rdd.partitionBy(new RangePartitioner(2,rdd))//将数据按照键的字典顺序进行排序 再分区

	3.普通RDD
		(1)集合间的操作
			distinct 去重
				val rdd = sc.makeRDD(List(1,3,5,7,9,3,7,10,23,7));
				rdd.distinct
			union 并集 -- 也可以用++实现
				val rdd1 = sc.makeRDD(List(1,3,5));
				val rdd2 = sc.makeRDD(List(2,4,6,8));
				val rdd = rdd1.union(rdd2);
				val rdd = rdd1 ++ rdd2;
			intersection 交集
				val rdd1 = sc.makeRDD(List(1,3,5,7));
				val rdd2 = sc.makeRDD(List(5,7,9,11));
				val rdd = rdd1.intersection(rdd2);
			subtract 差集
				val rdd1 = sc.makeRDD(List(1,3,5,7,9));
				val rdd2 = sc.makeRDD(List(5,7,9,11,13));
				val rdd =  rdd1.subtract(rdd2);

		(2)collect 收集
			//将rdd分布式存储在集群中不同分区的数据 获取到一起 组成一个数组返回
			//要注意 这个方法将会把所有数据搞到一个机器内 容易造成内存的溢出 在生产环境下千万慎用
			rdd.collect

		(3)take 获取前几个数据
			val rdd = sc.makeRDD(List(52,31,22,43,14,35))
			rdd.take(2)

		(4)takeOrdered(n) 先将rdd中的数据进行升序排序 然后取前n个
			val rdd = sc.makeRDD(List(52,31,22,43,14,35))
			rdd.takeOrdered(3)

		(5)top(n) 先将rdd中的数据进行降序排序 然后取前n个
			val rdd = sc.makeRDD(List(52,31,22,43,14,35))
			rdd.top(3)	
		
		(6)map 将函数应用到rdd的每个元素中
			val rdd = sc.makeRDD(List(1,3,5,7,9))
			rdd.map(_*10)

		(7)filter 用来从rdd中过滤掉不符合条件的数据
			val rdd = sc.makeRDD(List(1,3,5,7,9));
			rdd.filter(_<5);

		(8)flatMap 扁平map处理
			val rdd = sc.makeRDD(List("hello world","hello count","world spark"),2)
			//Array(Array(hello, world), Array(hello, count), Array(world, spark))
			rdd.map(_.split{" "})
			//Array[String] = Array(hello, world, hello, count, world, spark)
			rdd.flatMap(_.split{" "})
		
		(9)cache 缓存
			可以为rdd设置缓存
			rdd.cache()
			这样当未来需要重新获取rdd中的数据时 不需要重新创建 直接可以从还从中得到数据从而提升效率
			这个缓存信息可以在spark的ui管理界面中查看到
			
		(10)persist 缓存
			import org.apache.spark.storage.StorageLevel
			rdd.persist(StorageLevel.MEMORY_ONLY_SER)

		(11)cartesian 笛卡尔积
			val rdd1 = sc.makeRDD(List(1,2,3))
			val rdd2 = sc.makeRDD(List("a","b"))
			rdd1.cartesian(rdd2);

		(12)coalesce(n,true/false) 扩大或缩小分区
			val rdd = sc.makeRDD(List(1,2,3,4,5),2)
			rdd9.coalesce(3,true);//如果是扩大分区 需要传入一个true 表示要重新shuffle
			rdd9.coalesce(2);//如果是缩小分区 默认就是false 不需要明确的传入

		(13)repartition(n) 等价于上面的coalesce
			
		(14)count 统计rdd中元素的个数
			val rdd = sc.makeRDD(List(1,2,3,4,5),2)
			rdd.count

		(15)countApprox 计算一个近似值
			可以传入一个时间毫秒值 时间越长求出的结果精度越高
			val rdd = sc.makeRDD(1 to 1000000000,5)
			rdd.countApprox(5 * 1000)

		(16)mapPartitionsWithIndex 分别遍历分区做不同的处理
			val rdd = sc.makeRDD(List(1,2,3,4,5),2);
			rdd.mapPartitionsWithIndex((i,iter)=>{
				var list = List[String]()
				if(i==0){
					while(iter.hasNext){
						list = list :+ (iter.next + "a")
					}
				} else {
					while(iter.hasNext){
						list = list :+ (iter.next + "b")
					}
				}
				list.iterator
			});
			
		(17)saveAsTextFile 按照文本方式保存分区数据
			val rdd = sc.makeRDD(List(1,2,3,4,5),2);
			rdd.saveAsTextFile("/root/work/aaa")
		
		(18)textFile 读取文件数据成为rdd
			val rdd = sc.textFile("/root/work/words.txt",2);

		(19)sortBy 将rdd中的数据经过函数处理根据处理结果将原始数据进行排序
			val rdd = sc.makeRDD(List(123,324,1,35,23,5));	
			rdd.sortBy(x=>x);//升序
			rdd.sortBy(x=>x,false);//降序
			rdd.sortBy(x=>{if(x<100)x*1000 else x})

		(20)zip 拉链操作
			val rdd1 = sc.makeRDD(List("aaa","bbb","ccc"));
			val rdd2 = sc.makeRDD(List(1,2,3));
			rdd1.zip(rdd2)


		**案例：通过rdd实现统计文件中的单词数量
			sc.textFile("/root/work/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("/root/work/wcresult")

三、park执行原理 - RDD - 懒执行 DAG有向无环图 流水线优化 shuffle操作
	1.懒惰式命令
		val rdd1 = sc.makeRDD(List(1,2,3,4,5,6),2);
		val rdd2 = rdd1.map(x=>{println("abc");x+1;})//并没有产生任何的打印
		rdd2.collect//打印了abc 及 处理完的结果
		这个实验说明 默认 spark会有懒执行的机制 并不是每一步操作都会立即发生 可以能有延迟执行的效果

		问题是 什么样的方法会懒执行 什么样的方法会立即执行呢？

	2.Transformation/Action类型的方法
		rdd相关的方法可以分为Transformation类型的方法 和 Action两种类型

		如果一个方法是由RDD调用 执行后产生 另一个RDD 则这个方法属于Transformation方法
		如果一个方法是由RDD调用 执行后不是产生另一个RDD 而是产生一个非RDD的结果 则这个方法是Action类型的方法

		Transformation类型的方法 会 懒执行
		Action类型的方法 会 立即执行 执行当前操作和之前还未执行的懒执行操作
		
		为什么SPark要采用懒执行机制呢？
			Spark会将所有连续的懒执行的操作都不立即执行 而是根据执行计划组建出一个执行的有向无环图 称为DAG 直到遇到Action类型的操作 整个DAG有向无环图 才真正去执行
			这样的目的在于 在DAG有向无环图执行的内部可以执行流水线优化减少shuffle的过程 提高执行效率。

	3.窄依赖 / 宽依赖
		rdd之间的依赖： 整个DAG有向无环图的执行 其实就是处理rdd为另一个rdd的过程 这个过程中父rdd和子rdd之间是有关系的这种关系称之为子rdd对父rdd依赖 这种依赖是通过在子rdd中保存父rdd的血缘关系了来实现。
		rdd之间的依赖又分为窄依赖和宽依赖

		窄依赖：父RDD中的所有的分区都只面向一个子RDD中的分区
		宽依赖：父RDD中有分区面向多个子RDD中的分区

		窄依赖可以省略shuffle的过程 执行效率可以大大提高

		而如果整个DAG中存在多个连续的窄依赖 则可以将这些连续的窄依赖整合到一起连续执行 中间不执行shuffle 从而提高效率 这样的优化方式称之为流水线优化

		整个spark在执行DAG的过程中 提升性能的关键就是 尽力的去应用流水线优化 减少shuffle的过程
		
	4.spark的处理rdd的过程
		spark在遇到Transformation类型操作时都不会立即执行 而是懒执行 若干步的Transformation类型的操作后 一旦遇到Action类型操作时 必须要执行了 这时将所有之前的Transformation类型的操作和当前Action类型的操作组成一个DAG有向无环图 。
		再从Action方法向前回溯 如果遇到的是窄依赖则应用流水线优化 继续向前找 直到遇到宽依赖 无法实现优化 则将这一次段执行过程组装为一个stage 再从当前宽依赖开始继续向前找 重复刚才的步骤 从而将整个DAG划分为若干的stage。
		在stage内部可以执行流水线优化 而在stage之间没办法执行流水线优化 必然会有shuffle 但是这种机制已经尽力的去避免了shuffle
		最终
		一个DAG对应一个Spark的Job  而其中划分出来的stage对应的就是job当中的task 而又由于rdd中可能有多个分区 这个task可能有多个实例来分布式的并发处理数据
		这样 减少了 task的数量 减少了shuffle的过程 - 减少了数据落地的情况 和 由于shuffle的全局栅栏造成对性能的影响。

		这就是为什么spark比hadoop快的原因

	5.spark中的shuffle
		spark中一旦遇到宽依赖就需要进行shuffle的操作
		所谓的shuffle的操作的本质就是将数据汇总后重新分发的过程
		这个过程数据要汇总再分发 数据量可能很大所以不可避免的需要进行数据落磁盘的操作 会降低程序的性能
		所以spark并不是完全内存不读写磁盘 只能说它尽力避免这样的过程来提高效率 
		spark中的shuffle 在早期的版本中 会产生多个临时文件 但是这种多临时文件的策略 造成大量文件的同时的读写 磁盘的性能被分摊给多个文件 每个文件读写效率都不高 影响spark的执行效率
		所以在后续的spark中(1.5.0之后的版本)的shuffle中 只会产生一个文件 并且数据会经过排序 再附加索引信息 减少了文件的数量 并通过排序索引的方式提升了性能
		但是这种方式也有缺点 比如 排序 和查询索引都是需要时间消耗的 只能说是spark设计者的一种取舍 并不是完美的方案。

	6.案例：
		//窄依赖 不会shuffle
		val rdd1 = sc.makeRDD(List(1,2,3,4,5,6,7,8,9),3)	
		rdd1.map(_*10);

		//宽依赖 会发生shuffle
		val rdd1 = sc.makeRDD(List( ("cat",1),("cat",3),("dog",1),("tiger",1)),2)
		val rdd2 = sc.makeRDD(List( ("cat",2),("dog",2),("tiger",2)),2)
		rdd1.join(rdd2);
		
		
		//wordcount案例
		val rdd = sc.textFile("/root/work/words.txt")
		.flatMap(_.split(" "))
		.map { x=> (x,1) }
		.groupBy { x=> x._1 }
		.mapValues{ list => list.map { t => t._2 }.reduce{ (x,y) => x+y} }

四、Spark的可靠性保证
	spark具有可靠性的保证
	这个可靠性的保证在RDD层面有所体现 体现在RDD中存在血缘信息 保存了父RDD相关的信息
	当子RDD在处理的过程中产生问题时 可以通过血缘关系回溯找到 最初的数据 来重新执行 保证数据不会丢失

	注意 并不是子RDD出现问题 找到当前子RDD的父RDD就可以的 因为父RDD中的数据 可能已经释放掉了 需要一直回溯到 最初的位置 重新计算
	这样 重新计算时 整个DAG都要从头执行 效率低

	而如果在中间环节中设置过缓存 则在回溯时 找到缓冲中的数据 可以使用 就不需要再在向前寻找 提高效率

	这是缓存在可靠性保证中提升效率的体现

五、spark的集群安装
	配置集群
		在集群的每个节点中都中解压spark	
		在spark-env.sh中配置SPARK_LOCAL_IP=当前主机地址

	启动集群
		在master节点中通过如下命令启动master
			sbin/start-master.sh -h xxmasteripxx
		在slave节点通过如下命令启动slave
			sbin/start-slave.sh spark://xxxmasteripxxx:7077
		通过浏览器访问管理界面
			http://xxxmasteripxxx:8080
		通过客户端连接
			bin/spark-shell.sh --master spark://xxxmasteripxxx:7077

	在集群中读取文件：
		sc.textFile("/root/work/words.txt")
			默认读取本机数据 这种方式需要在集群的每台机器上的对应位置上都一份该文件 浪费磁盘
			所以应该通过hdfs存储数据
		sc.textFile("hdfs://hadoop01:9000/mydata/words.txt");

		**可以在spark-env.sh 中配置选项 HADOOP_CONF_DIR 配置为hadoop的etc/hadoop的地址 使默认访问的是hdfs的路径
		**如果修改默认地址是hdfs地址 则如果想要访问文件系统中的文件 需要指明协议为file 例如 sc.text("file:///xxx/xx")

六、spark任务提交
	1.创建spark的项目
		在scala中创建项目 导入spark相关的jar包
		开发spark相关代码：
			//创建配置
			val conf = new SparkConf();
			conf.setAppName("SparkDemo1")      //创建应用程序first
			conf.setMaster("spark://hadoop01:7077")
			//conf.set("spark.shuffle.manager","hash")

			//基于配置生成sc
			val sc = new SparkContext(conf);

			//基于sc开发spark代码
			val rdd1 = sc.textFile("/mydata/words.txt", 2);
			val rdd2 = rdd1.map((_,1)).groupBy(_._1).mapValues(_.map(_._2).reduce(_+_));

			//将结果写出
			rdd2.saveAsTextFile("/mydata/results");

	2.将写好的项目打成jar
		上传到服务器
		通过命令将jar提交到spark中运行
			./spark-submit --class SparkDemo1 /root/work/sparkDemo1.jar --executor-memory 512m
		

	===扩展理解=============================================
		RDD 全称为 弹性分布式数据集 分布式的数据集合 好像给人的感觉是 所有的数据分布式的存储在集群不同节点的内存中进行运算 从逻辑上这样理解没有大问题 但事实上物理实现并不是这样 
		实际上 RDD从物理结构上来看 并不是一个分布式内存中存放完整数据的集合 真正的计算被转换为DAG有向无环图 执行时 数据不停的从来源流入 经由每个算子进行计算 最多写出到结果中 并不会真正的在内存中 存储全量的数据成为一个RDD 而是流式的处理 这样内存的消耗其实是非常小的
		但是并不是所有的操作都可以按照如上的流式的方式执行，比如 groupBy join等操作 需要在全量数据上进行处理 这时只能将将全量数据 持久化后再处理 其实就是shuffle的过程，这其实就是流水线优化 和 shuffle的本质
	=======================================================
	
七、SparkSql
	1.概述
		SparkSql的前身叫做Shark。
		Shark是在hive的基础上 替换了 其中的计算引擎 从mr换成spark从而提升了效率
		但是之后 为了减少对hive的依赖 spark的设计者 放弃了Shark从新开发了SparkSql
		由于底层的Spark比mr的效率要高很多 所以SparkSql的效率也要比hive高很多
		现在越来越流行起来了

	2.SparkSql使用 - 创建DataFrame
		SparkSql中有一个核心的数据结构叫做DataFrame 本质上是对RDD的一个封装 其中采用类似表的结构来存储数据

		(1)将rdd转换为df
			案例：
				val rdd = sc.makeRDD(List(1,2,3,4,5));	
				val df1 = rdd.toDF("id");
				df1.show()

			案例：
				val rdd = sc.makeRDD(List((1,"zhang",19,"bj",1000000),(2,"wang",29,"sh",100),(3,"li",49,"sz",999)));
				val df2 = rdd.toDF("id","name","age","addr","salary");
				df2.show()

		(2)将txt文件转换为df
			txt格式的文件不能直接被转换为df 需要先转换为rdd 再转换

			案例：
				val rdd = sc.textFile("file:///root/work/words.txt");	
				val df3 = rdd.flatMap(_.split(" "))
							.map((_,1))
							.reduceByKey(_+_)
							.toDF("word","count");

		(3)将json文件转换为df
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df4 = sqlContext.read.json("file:///root/work/people.json");
				
		(4)将parquet文件转换df
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df5 =  sqlContext.read.parquet("file:///root/work/users.parquet");

		(5)利用jdbc将关系型数据中的数据转换为df
			在启动时加载数据库驱动包
				./spark-shell --master=local --driver-class-path=/root/work/mysql-conneva-5.1.38-bin.jar
			执行代码
				val sqlContext = new org.apache.spark.sql.SQLContext(sc);
				val prop = new java.util.Properties
				prop.put("user","root")
				prop.put("password","root")
				val df6 = sqlContext.read.jdbc("jdbc:mysql://hadoop01:3306/sparkdb","tab1",prop)

	3.SparkSql使用 - 使用DataFrame - 通过方法来使用
		(1)查询
			df.select("id","name").show();

		(2)带条件的查询
			df.select($"id",$"name").where($"name" === "b").show()
			
		(3)排序查询
			orderBy($"列名")  升序排列
			orderBy($"列名".desc)  降序排列
			orderBy($"列1" , $"列2".desc) 按两列排序

			df.select($"id",$"name").orderBy($"name".desc).show
			df.select($"id",$"name").sort($"name".desc).show

		(4)分组查询
			groupBy("列名", ...).max(列名) 求最大值
			groupBy("列名", ...).min(列名) 求最小值
			groupBy("列名", ...).avg(列名) 求平均值
			groupBy("列名", ...).sum(列名) 求和
			groupBy("列名", ...).count() 求个数
			groupBy("列名", ...).agg 可以将多个方法进行聚合

			val rdd = sc.makeRDD(List((1,"a","bj"),(2,"b","sh"),(3,"c","gz"),(4,"d","bj"),(5,"e","gz")));
			val df = rdd.toDF("id","name","addr");
			df.groupBy("addr").count().show()

		(5)连接查询
			val dept=sc.parallelize(List((100,"财务部"),(200,"研发部"))).toDF("deptid","deptname")
			val emp=sc.parallelize(List((1,100,"张财务"),(2,100,"李会计"),(3,200,"王艳发"))).toDF("id","did","name")
			dept.join(emp,$"deptid" === $"did").show
			dept.join(emp,$"deptid" === $"did","left").show
			dept.join(emp,$"deptid" === $"did","right").show

		(6)执行运算
			val df = sc.makeRDD(List(1,2,3,4,5)).toDF("num");
			df.select($"num" * 100).show

		(7)使用列表
			val df = sc.makeRDD(List(("zhang",Array("bj","sh")),("li",Array("sz","gz")))).toDF("name","addrs")
			df.selectExpr("name","addrs[0]").show

		(8)使用结构体
			{"name":"陈晨","address":{"city":"西安","street":"南二环甲字1号"}}
			{"name":"娜娜","address":{"city":"西安","street":"南二环甲字2号"}}

			val df = sqlContext.read.json("file:///root/work/users.json")
			dfs.select("name","address.street").show

		(9)其他
			df.count//获取记录总数
			val row = df.first()//获取第一条记录
			val value = row.getString(1)//获取该行指定列的值
			df.collect //获取当前df对象中的所有数据为一个Array 其实就是调用了df对象对应的底层的rdd的collect方法


	4.SparkSql使用 - 使用DataFrame - 通过sql语句来调用
		(0)创建表
			创建临时表 - 会话结束表被删除
				df.registerTempTable("tabName")
			创建持久表 - 会话结束表也不删除
				df.saveAsTable("tabName")
		(1)查询
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df = sc.makeRDD(List((1,"a","bj"),(2,"b","sh"),(3,"c","gz"),(4,"d","bj"),(5,"e","gz"))).toDF("id","name","addr");
			df.registerTempTable("stu");
			sqlContext.sql("select * from stu").show()
			
		(2)带条件的查询
			val df = sc.makeRDD(List((1,"a","bj"),(2,"b","sh"),(3,"c","gz"),(4,"d","bj"),(5,"e","gz"))).toDF("id","name","addr");
			df.registerTempTable("stu");
			sqlContext.sql("select * from stu where addr = 'bj'").show()
		
		(3)排序查询
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df = sc.makeRDD(List((1,"a","bj"),(2,"b","sh"),(3,"c","gz"),(4,"d","bj"),(5,"e","gz"))).toDF("id","name","addr");
			df.registerTempTable("stu");
			sqlContext.sql("select * from stu order by addr").show()

		(4)分组查询
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df = sc.makeRDD(List((1,"a","bj"),(2,"b","sh"),(3,"c","gz"),(4,"d","bj"),(5,"e","gz"))).toDF("id","name","addr");
			df.registerTempTable("stu");
			sqlContext.sql("select addr,count(*) from stu group by addr").show()

		(5)连接查询
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val dept=sc.parallelize(List((100,"财务部"),(200,"研发部"))).toDF("deptid","deptname")
			val emp=sc.parallelize(List((1,100,"张财务"),(2,100,"李会计"),(3,200,"王艳发"))).toDF("id","did","name")
			dept.registerTempTable("deptTab");
			emp.registerTempTable("empTab");
			sqlContext.sql("select deptname,name from deptTab inner join empTab on deptTab.deptid = empTab.did").show()

		(6)执行运算
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df = sc.makeRDD(List(1,2,3,4,5)).toDF("num");
			df.registerTempTable("tabx")
			sqlContext.sql("select num * 100 from tabx").show();

		(7)分页查询
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			val df = sc.makeRDD(List(1,2,3,4,5)).toDF("num");
			df.registerTempTable("tabx")
			sqlContext.sql("select * from tabx limit 3").show();

		(8)查看表
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);

			val df1 = sc.makeRDD(List(1,2,3,4,5)).toDF("num");
			df1.registerTempTable("tabx1")
			
			val df2 = sc.makeRDD(List(1,2,3,4,5)).toDF("num");
			df1.saveAsTable("tabx2")
			
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			sqlContext.sql("show tables").show

		(9)类似hive方式的操作
			val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
			hiveContext.sql("CREATE TABLE IF NOT EXISTS zzz (key INT, value STRING) row format delimited fields terminated by '|'")
			hiveContext.sql("LOAD DATA LOCAL INPATH 'file:///root/work/hdata.txt' INTO TABLE zzz")
			val df5 = hiveContext.sql("select key,value from zzz")


		(10)案例
			val sqlContext = new org.apache.spark.sql.SQLContext(sc);
			 val df = sc.textFile("file:///root/work/words.txt").flatMap{ _.split(" ") }.toDF("word")
			 df.registerTempTable("wordTab")
			 sqlContext.sql("select word,count(*) from wordTab group by word").show

	5.通过api使用sparksql
		object SparkSqlDemo1 {
			def main(args: Array[String]): Unit = {
				//创建配置
				//val conf = new SparkConf();
				//conf.setAppName("SparkDemo1")      //创建应用程序first
				//conf.setMaster("spark://hadoop01:7077")
				//conf.set("spark.shuffle.manager","hash")

				//基于配置生成sc
				val sc = new SparkContext();

				//创建出sqlContext
				val sqlContext = new SQLContext(sc);

				import sqlContext.implicits._
				val rdd = sc.makeRDD(List((1,"zhang"),(2,"li"),(3,"wang")));
				val df = rdd.toDF("id","name");

				df.registerTempTable("tabx");

				val df2 = sqlContext.sql("select * from tabx order by name");

				val rdd2 = df2.toJavaRDD;
				rdd2.saveAsTextFile("file:///root/work/df.txt");
				}
			}

		打成jar包 提交到spark中运行
		
八、SparkStreaming
	1.概述
		Spark Streaming是一种构建在Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力，以吞吐量高和容错能力强著称。

		SparkStreaming 将输入的数据 按照时间为单位进行切片 切出一个个的批 称之为DStream 而DStream本质上就是Spark中的RDD 对DStream的处理最终会被翻译成对底层RDD的处理

		SparkStreaming在处理DStream数据时 是按照串行化的方式进行处理的。这也就意味着上一个DStream在当前算子中未处理完成时 下一个DStream即使到来也要阻塞等待。 而DStream内部本质上是RDD 本身就是分布式的数据集 可以分布式的处理。 所以SparkStreaming中对数据的并发的处理是体现在DStream对应的RDD本身的并发上 ， 而放弃了批之间的并发。放弃了批之间的并发后 虽然造成的延时性上的一些损失 但是在可靠性保证 并发控制 程序开发复杂度的降低上 都带来了好处。我们在使用SparkStreaming时 应该合理的配置切片时间 和 每个算子的复杂程度 尽力的让每个算子都可以在切片时间内将数据处理完 这样 可以减少数据可能的堆积 以及 算子闲置的可能 实现最优的并发。

		相对于Storm SparkStreaming的优势并不体现在低延时 而是在高吞吐量上有自己独特的优势。

	2.入门案例
		import org.apache.spark.streaming._
		val ssc = new StreamingContext(sc,Seconds(1));
		val lines = ssc.textFileStream("file:///root/work/streamData");
		val words = lines.flatMap(_.split(" "));
		val wordCounts = words.map((_,1)).reduceByKey(_+_);
		wordCounts.print();
		ssc.start();

		经过测试如上的代码确实可以监控指定的文件夹处理其中产生的新的文件
		经过测试 发现 数据每次都会重新进行计算 每个block都会有一个独立的结果
		而如果需要对历史数据进行累计处理 该怎么做呢
		SparkStreaming提供了checkPoint机制 通过在临时文件中存储中间数据 为历史数据累计处理提供了可能性

			import org.apache.spark.streaming._
			val ssc = new StreamingContext(sc,Seconds(1));
			ssc.checkpoint("file:///root/work/streamCheckPoint");
			val lines = ssc.textFileStream("file:///root/work/streamData");
			val words = lines.flatMap(_.split(" "));
			val wordCounts = words.map((_,1)).updateStateByKey{
			  (seq, op:Option[Int]) => { Some(seq.sum + op.getOrElse(0)) }
			}
			wordCounts.print();
			ssc.start();

		上面的例子实现了数据的累计统计
		但是这上面的例子里所有的数据不停的累计 一直累计下去
		很多的时候我们要的也不是这样的效果 我们希望能够每隔一段时间重新统计下一段时间的数据
		这样的功能可以通过滑动窗口的方式来实现
		在DStream中提供了如下的和滑动窗口相关的方法：
			window(windowLength, slideInterval)
			countByWindow(windowLength, slideInterval)
			reduceByWindow(func, windowLength, slideInterval)
			reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])
			reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])
			countByValueAndWindow(windowLength, slideInterval, [numTasks])
		可以通过以上机制改造案例
			import org.apache.spark.streaming._
			val ssc = new StreamingContext(sc,Seconds(1));
			ssc.checkpoint("file:///root/work/streamCheckPoint");
			val lines = ssc.textFileStream("file:///root/work/streamData");
			val words = lines.flatMap(_.split(" "));
			words.map((_,1)).reduceByKeyAndWindow( (x:Int,y:Int)=>x+y, Seconds(5), Seconds(5) ).print
			ssc.start();

	3.输入输出相关方法
		输入控制
			SparkStreaming可以从多种输入源获取数据
			文件系统
				ssc.textFileStream
			hdfs
			kafka
				KafkaUtils.createStream(ssc, "localhost:2181", "group1", Map("jt"->1))
			数据库
			网络
				ssc.socketTextStream
			。。。。。
		
		输出控制
			print()	Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. 
			Python API This is called print() in the Python API.

			saveAsTextFiles(prefix, [suffix])	Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]".

			saveAsObjectFiles(prefix, [suffix])	Save this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". 
			Python API This is not available in the Python API.

			saveAsHadoopFiles(prefix, [suffix])	Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". 
			Python API This is not available in the Python API.

			foreachRDD(func)	The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.

			改造如上案例 将数据存储到文件中
				import org.apache.spark.streaming._
				val ssc = new StreamingContext(sc,Seconds(1));
				ssc.checkpoint("file:///root/work/streamCheckPoint");
				val lines = ssc.textFileStream("file:///root/work/streamData");
				val words = lines.flatMap(_.split(" "));
				words.map((_,1)).reduceByKeyAndWindow( (x:Int,y:Int)=>x+y, Seconds(10), Seconds(10) ).saveAsTextFiles("file:///root/work/ssresults/ssx")
				ssc.start();
九、数据挖掘分析
	1.概述
		数据分析 - 数据分析师
			基于数据对数据中包含的业务信息做统计分析。是对历史数据的总结和分析。
			数学功底（统计学） 业务的理解  数据库 数据仓库 sql excel 更专业的工具。
		数据挖掘 - 数据挖掘工程师
			基于数据对数据中包含的数据和业务规律做总结，从而做出预测为未来的行为提供指导。是基于挖掘出的规律对未来做预测。
			数学的功底（统计学 高等数学 离散数学 概率统计。。。） 算法功底 大数据的处理能力
		数学建模 - 数据科学家 建模工程师
			是数据挖掘的基础 是数据挖掘的前期步骤 通过算法通过海量数据的处理能力 从海量的数据中 建立数学模型（一些表明数据规	律的公式）描述数据特点 总结出这样的模型后可以基于这个模型对未来的可能性进行预测
			数学的功底（统计学 高等数学 离散数学 概率统计。。。） 算法功底
		大数据项目的实现 - 大数据开发工程

	2.数据建模 解决房价预测问题	
		//所有数据	
		F = load("prices.txt");
		//第一个矩阵
		L = [ones(22,1),F(:,2)]
		//第二个矩阵
		R = F(:,1)	
		//准备向量 - 这个之随便取
		Q = [0;6]	
		//进行误差计算
		J = sum(L*Q-R).^2/(2*22)	
		

		function Z = costFunction(Q0,Q1,L,R)
			Z = zeros(length(Q0),length(Q1));
			m = length(R);
			for i=1:length(Q0)
				for j=1:length(Q1)
					Z(i,j) = sum((L*[Q0(i);Q1(j)]-R).^2)/(2*m);
				end
			end
			Z = Z';
		end


		Q0 = [-200:10:300]
		Q1 = [2:0.2:12]
		J=costFunction(Q0,Q1,L,R)		

		plot3(J)


	=========================================================================================================

		function [z,J_his] = descentFunction(z,X,y,a,iters)
			J_his = zeros(iters,1);
			m = length(y);		% 样本数
			n = length(z);		
			t = zeros(n,1);
			for iter = 1:iters
				for i = 1:n
					% 变化率
					t(i) = (a/m)*(X*z-y)'*X(:,i);
				end;
				for i = 1:n
					z(i) = z(i) - t(i);
				end;
				J_his(iter) = sum((X*z-y).^2)/(2*m);
			end
		end


		//所有数据	
		F = load("prices.txt");
		//第一个矩阵
		L = [ones(22,1),F(:,2)]
		//第二个矩阵
		R = F(:,1)	
		//指定学习速率
		z=[0;6]
		a=0.0001
		iters=1
		[z,j] = descentFunction(z,L,R,a,iters)
		
		3220

		y = -0.022307 * 1 + 6.945142 * 200


	3.SparkMlib机器学习
		向量
				import org.apache.spark.mllib.linalg.Vectors
			稠密：
				Vectors.dense(1,3,5,7,9)
			稀疏：
				Vectors.sparse(5,Array(0,3,4),Array(10,100,1000))

		LabeledPoint
			只有先把数据转变成LabeledPoint才能使用SparkMlib提供的函数
			一个LabeledPoint由一个类标签（Double型）和一个向量（密集或者稀疏）组成
			
			构建LabeledPoint：
				import org.apache.spark.mllib.regression.LabeledPoint
				LabeledPoint(4,Vectors.dense(1,2,3))

		SparkMlib中通过线性回归预测房价
			val rdd = sc.textFile("file:///root/work/prices.txt");
			val rdd2 = rdd.map(_.split("\t"))
			val rdd3 = rdd2.map((x)=>{LabeledPoint(x(0).toDouble,Vectors.dense(1.0,x(1).toDouble))})

			import org.apache.spark.mllib.regression.LinearRegressionModel
			import org.apache.spark.mllib.regression.LinearRegressionWithSGD

			val model = LinearRegressionWithSGD.train(rdd3, 1000, 0.0001)

			model.weights

			model.predict(Vectors.dense(1.0,200.0))

